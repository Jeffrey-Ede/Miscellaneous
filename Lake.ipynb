{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lake.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOuTyFoEx3WW8wfq4vUNuR7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeffrey-Ede/Miscellaneous/blob/master/Lake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exe_EatkVce6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKXq96a3SihC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee82c366-87e8-433d-fe7a-03f1cbcf0673"
      },
      "source": [
        "# #Install auto-sklearn with build dependencies\r\n",
        "\r\n",
        "!sudo apt-get install build-essential swig \r\n",
        "!curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install \r\n",
        "!pip install auto-sklearn==0.10.0\r\n",
        "\r\n",
        "# !sudo apt-get install build-essential swig\r\n",
        "# !pip install auto-sklearn==0.11.1\r\n",
        "\r\n",
        "#!pip install scikit-optimize"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   207  100   207    0     0   6468      0 --:--:-- --:--:-- --:--:--  6468\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (51.3.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.14.1) (1.19.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Collecting scikit-learn<0.25.0,>=0.24.0\n",
            "  Using cached https://files.pythonhosted.org/packages/e2/4c/6111b9a325f29527d7f262e2ee8c730d354b47a728d955e186dacad57a0d/scikit_learn-0.24.1-cp36-cp36m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.25.0,>=0.24.0) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.25.0,>=0.24.0) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.25.0,>=0.24.0) (1.19.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.25.0,>=0.24.0) (2.1.0)\n",
            "\u001b[31mERROR: auto-sklearn 0.10.0 has requirement scikit-learn<0.23,>=0.22.0, but you'll have scikit-learn 0.24.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.1\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.6/dist-packages (2021.1.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from dask) (3.13)\n",
            "Requirement already satisfied: distributed>=2.2.0 in /usr/local/lib/python3.6/dist-packages (2021.1.1)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (51.3.3)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (1.6.0)\n",
            "Requirement already satisfied: contextvars; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (2.4)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (5.4.8)\n",
            "Requirement already satisfied: tornado>=5; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (5.1.1)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (0.11.1)\n",
            "Requirement already satisfied: dask>=2020.12.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (2021.1.1)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (2.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (3.13)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (1.0.2)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (1.7.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.2.0) (2.3.0)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars; python_version < \"3.7\"->distributed>=2.2.0) (0.14)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.6/dist-packages (from zict>=0.1.3->distributed>=2.2.0) (1.0.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Collecting pandas>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/e2/00cacecafbab071c787019f00ad84ca3185952f6bb9bca9550ed83870d4d/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5MB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0) (1.15.0)\n",
            "\u001b[31mERROR: auto-sklearn 0.10.0 has requirement pandas<1.0, but you'll have pandas 1.1.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: auto-sklearn 0.10.0 has requirement scikit-learn<0.23,>=0.22.0, but you'll have scikit-learn 0.24.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pandas\n",
            "  Found existing installation: pandas 0.25.3\n",
            "    Uninstalling pandas-0.25.3:\n",
            "      Successfully uninstalled pandas-0.25.3\n",
            "Successfully installed pandas-1.1.5\n",
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.6/dist-packages (2.5.0)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.14 in /usr/local/lib/python3.6/dist-packages (0.4.17)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.14) (0.29.21)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.14) (1.19.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.14) (2.4.7)\n",
            "Requirement already satisfied: pynisher>=0.6.3 in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.6.3) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.6.3) (51.3.3)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.6.3) (0.16)\n",
            "Requirement already satisfied: pyrfr<0.9,>=0.7 in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: smac<0.14,>=0.13.1 in /usr/local/lib/python3.6/dist-packages (0.13.1)\n",
            "Requirement already satisfied: pynisher>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from smac<0.14,>=0.13.1) (0.6.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from smac<0.14,>=0.13.1) (1.0.0)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.14 in /usr/local/lib/python3.6/dist-packages (from smac<0.14,>=0.13.1) (0.4.17)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from smac<0.14,>=0.13.1) (5.4.8)\n",
            "Requirement already satisfied: pyrfr>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from smac<0.14,>=0.13.1) (0.8.0)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.6/dist-packages (from smac<0.14,>=0.13.1) (2021.1.1)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from smac<0.14,>=0.13.1) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from smac<0.14,>=0.13.1) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from smac<0.14,>=0.13.1) (0.24.1)\n",
            "Requirement already satisfied: lazy-import in /usr/local/lib/python3.6/dist-packages (from smac<0.14,>=0.13.1) (0.2.2)\n",
            "Requirement already satisfied: distributed in /usr/local/lib/python3.6/dist-packages (from smac<0.14,>=0.13.1) (2021.1.1)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.1->smac<0.14,>=0.13.1) (0.16)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.1->smac<0.14,>=0.13.1) (51.3.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.14->smac<0.14,>=0.13.1) (2.4.7)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.14->smac<0.14,>=0.13.1) (0.29.21)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from dask->smac<0.14,>=0.13.1) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.22.0->smac<0.14,>=0.13.1) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from lazy-import->smac<0.14,>=0.13.1) (1.15.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed->smac<0.14,>=0.13.1) (1.7.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed->smac<0.14,>=0.13.1) (1.0.2)\n",
            "Requirement already satisfied: contextvars; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from distributed->smac<0.14,>=0.13.1) (2.4)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from distributed->smac<0.14,>=0.13.1) (1.6.0)\n",
            "Requirement already satisfied: tornado>=5; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from distributed->smac<0.14,>=0.13.1) (5.1.1)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.6/dist-packages (from distributed->smac<0.14,>=0.13.1) (7.1.2)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from distributed->smac<0.14,>=0.13.1) (2.0.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.6/dist-packages (from distributed->smac<0.14,>=0.13.1) (0.11.1)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.6/dist-packages (from distributed->smac<0.14,>=0.13.1) (2.3.0)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars; python_version < \"3.7\"->distributed->smac<0.14,>=0.13.1) (0.14)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.6/dist-packages (from zict>=0.1.3->distributed->smac<0.14,>=0.13.1) (1.0.1)\n",
            "Requirement already satisfied: auto-sklearn==0.10.0 in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (1.4.1)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (2021.1.1)\n",
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (2.5.0)\n",
            "Requirement already satisfied: pynisher>=0.4.2 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (0.6.3)\n",
            "Requirement already satisfied: pyrfr<0.9,>=0.7 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (0.8.0)\n",
            "Requirement already satisfied: ConfigSpace<0.5,>=0.4.14 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (0.4.17)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (1.0.0)\n",
            "Collecting scikit-learn<0.23,>=0.22.0\n",
            "  Using cached https://files.pythonhosted.org/packages/5e/d8/312e03adf4c78663e17d802fe2440072376fee46cada1404f1727ed77a32/scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting pandas<1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (51.3.3)\n",
            "Requirement already satisfied: distributed in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (2021.1.1)\n",
            "Requirement already satisfied: smac<0.14,>=0.13 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (0.13.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (1.19.5)\n",
            "Requirement already satisfied: lockfile in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (0.12.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from auto-sklearn==0.10.0) (3.13)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.2->auto-sklearn==0.10.0) (5.4.8)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher>=0.4.2->auto-sklearn==0.10.0) (0.16)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn==0.10.0) (0.29.21)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn==0.10.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas<1.0->auto-sklearn==0.10.0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<1.0->auto-sklearn==0.10.0) (2018.9)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.6/dist-packages (from distributed->auto-sklearn==0.10.0) (2.3.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed->auto-sklearn==0.10.0) (1.7.0)\n",
            "Requirement already satisfied: tornado>=5; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from distributed->auto-sklearn==0.10.0) (5.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from distributed->auto-sklearn==0.10.0) (1.6.0)\n",
            "Requirement already satisfied: contextvars; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from distributed->auto-sklearn==0.10.0) (2.4)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from distributed->auto-sklearn==0.10.0) (2.0.0)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.6/dist-packages (from distributed->auto-sklearn==0.10.0) (7.1.2)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed->auto-sklearn==0.10.0) (1.0.2)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.6/dist-packages (from distributed->auto-sklearn==0.10.0) (0.11.1)\n",
            "Requirement already satisfied: lazy-import in /usr/local/lib/python3.6/dist-packages (from smac<0.14,>=0.13->auto-sklearn==0.10.0) (0.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas<1.0->auto-sklearn==0.10.0) (1.15.0)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars; python_version < \"3.7\"->distributed->auto-sklearn==0.10.0) (0.14)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.6/dist-packages (from zict>=0.1.3->distributed->auto-sklearn==0.10.0) (1.0.1)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scikit-learn, pandas\n",
            "  Found existing installation: scikit-learn 0.24.1\n",
            "    Uninstalling scikit-learn-0.24.1:\n",
            "      Successfully uninstalled scikit-learn-0.24.1\n",
            "  Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "Successfully installed pandas-0.25.3 scikit-learn-0.22.2.post1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARyJEaWV7W3m"
      },
      "source": [
        "#Libraries that are likely to be useful\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import scipy\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "import os\r\n",
        "import itertools"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwcNF4iJAZQr",
        "outputId": "28a96d17-2060-482a-ab25-09cbe245e3d0"
      },
      "source": [
        "#Download data from my Google Drive\r\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Z84X9RLCaLyLSfATKEjgr9hGO7fm2ERk' -O acea.zip && mkdir -p ~/data/acea"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-30 13:21:25--  https://docs.google.com/uc?export=download&id=1Z84X9RLCaLyLSfATKEjgr9hGO7fm2ERk\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.20.100, 74.125.20.113, 74.125.20.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.20.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-a8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n9kuq0hckdf239fpvu84rlit3rvv9cl7/1612012875000/14151923763028593793/*/1Z84X9RLCaLyLSfATKEjgr9hGO7fm2ERk?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-01-30 13:21:27--  https://doc-0g-a8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n9kuq0hckdf239fpvu84rlit3rvv9cl7/1612012875000/14151923763028593793/*/1Z84X9RLCaLyLSfATKEjgr9hGO7fm2ERk?e=download\n",
            "Resolving doc-0g-a8-docs.googleusercontent.com (doc-0g-a8-docs.googleusercontent.com)... 74.125.20.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to doc-0g-a8-docs.googleusercontent.com (doc-0g-a8-docs.googleusercontent.com)|74.125.20.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 798780 (780K) [application/x-zip-compressed]\n",
            "Saving to: ‘acea.zip’\n",
            "\n",
            "acea.zip            100%[===================>] 780.06K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2021-01-30 13:21:27 (153 MB/s) - ‘acea.zip’ saved [798780/798780]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKOVRjnmD4lo"
      },
      "source": [
        "#Decompress datasets\r\n",
        "import zipfile\r\n",
        "\r\n",
        "with zipfile.ZipFile('acea.zip', 'r') as zip_ref:\r\n",
        "  zip_ref.extractall('acea')\r\n",
        "\r\n",
        "#Specify dataset file\r\n",
        "input_file = \"acea/Lake_Bilancino.csv\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bDA9LZ-7wGf"
      },
      "source": [
        "#NOTE: I'm not inputting information about seasonal variation, such as the month.\r\n",
        "#My thought is that it's probably implicit if you use a large enough time window\r\n",
        "#However, this probably merits further investigation\r\n",
        "\r\n",
        "NUM_INPUT_DATA = 64 #Days of data to input to model\r\n",
        "TRAIN_FRAC = 0.9 #Can be large if we're just investigating ability to generalize to 1-2 years\r\n",
        "\r\n",
        "from sklearn.preprocessing import QuantileTransformer\r\n",
        "\r\n",
        "from sklearn.model_selection import PredefinedSplit\r\n",
        "\r\n",
        "#Load data\r\n",
        "df = pd.read_csv(input_file, header=0)\r\n",
        "\r\n",
        "headers = list(df.columns.values)\r\n",
        "data = np.float32(df.values[:, 1:]) #Could probably use 16-bit\r\n",
        "\r\n",
        "#Only select data with at least one input feature\r\n",
        "#data = np.stack([x for x, f in zip(data, np.isfinite(data)) if np.sum(f[:6]) > 0])\r\n",
        "\r\n",
        "#If rows that contain no input feature are removed, then there's only one data column\r\n",
        "#with a NaN. That means that it's difficult to train the model to learn that something\r\n",
        "#like -1 means NaN. Avoid issue by stripping all rows with NaNs\r\n",
        "data = np.stack([x for x, f in zip(data, np.isfinite(data)) if np.sum(f[:6]) == 6])\r\n",
        "\r\n",
        "#Quantile transforms to normalize features to [0, 1]. For general case where data\r\n",
        "#contains missing values\r\n",
        "qts = []\r\n",
        "for i in range(data.shape[1]):\r\n",
        "  #Use histogram normalization to scale features to have values in [0, 1]\r\n",
        "  qt = QuantileTransformer(random_state=0)\r\n",
        "  col = data[:, i:i+1]\r\n",
        "  is_data = np.isfinite(col)\r\n",
        "\r\n",
        "  qt.fit_transform(np.expand_dims(col[is_data], -1))\r\n",
        "  qts.append(qt)\r\n",
        "\r\n",
        "  #Normalize data\r\n",
        "  col[is_data] = qt.transform(np.expand_dims(col[is_data], -1))[:, 0]\r\n",
        "\r\n",
        "  #In general, can replace missing data with -1\r\n",
        "  col[~is_data] = -1\r\n",
        "\r\n",
        "  data[:, i:i+1] = col\r\n",
        "\r\n",
        "\r\n",
        "#Dataset is tiny, so can increase its size 100 times in RAM without\r\n",
        "#any issue\r\n",
        "data = np.stack([data[i:-NUM_INPUT_DATA+i] for i in range(NUM_INPUT_DATA)], axis=1)\r\n",
        "\r\n",
        "#Test ability to generalize to latest data, which is at the bottom of the dataset\r\n",
        "#Note that this data is likely the most valuable for training to generalize beyond \r\n",
        "#the dataset, so another partition or validation fold may be preferable. \r\n",
        "train_size = int(TRAIN_FRAC*data.shape[0])\r\n",
        "\r\n",
        "x_train = data[:train_size,:,:-2]\r\n",
        "prev_y_train = data[:train_size,:NUM_INPUT_DATA-1,-2:]\r\n",
        "y_train = data[:train_size,NUM_INPUT_DATA-1,-2:]\r\n",
        "x_val = data[train_size:,:,:-2]\r\n",
        "prev_y_val = data[train_size:,:NUM_INPUT_DATA-1,-2:]\r\n",
        "y_val = data[train_size:,NUM_INPUT_DATA-1,-2:]\r\n",
        "\r\n",
        "# x = data[:,:,:-2]\r\n",
        "# y = data[:,:,-2:]\r\n",
        "\r\n",
        "test_fold = [-1 for _ in range(train_size)] + [0 for _ in range(NUM_INPUT_DATA-train_size)]\r\n",
        "ps = PredefinedSplit(test_fold=test_fold)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyYTbzIRXTlC",
        "outputId": "f9cb4719-0ee2-4446-9f46-564ffbb8ab97"
      },
      "source": [
        "#There might be a large difference in errors, so easiest to develop models for each \r\n",
        "#prediction separately to minimize development time\r\n",
        "\r\n",
        "from autosklearn.regression import AutoSklearnRegressor\r\n",
        "\r\n",
        "print(x_train.shape)\r\n",
        "print(prev_y_train.shape)\r\n",
        "\r\n",
        "#training data was originally shaped for CNN, so need to reshape \r\n",
        "x_train = np.concatenate(tuple([x_train[:,i,:] for i in range(NUM_INPUT_DATA)] +\r\n",
        "                               [prev_y_train[:,i,:] for i in range(NUM_INPUT_DATA-1)]), axis=-1)\r\n",
        "x_val = np.concatenate(tuple([x_val[:,i,:] for i in range(NUM_INPUT_DATA)] + \r\n",
        "                             [prev_y_val[:,i,:] for i in range(NUM_INPUT_DATA-1)]), axis=-1)\r\n",
        "\r\n",
        "pred_y_val = []\r\n",
        "for i in range(2):\r\n",
        "  automl = AutoSklearnRegressor(\r\n",
        "    time_left_for_this_task=5*60, per_run_time_limit=30, n_jobs=-1\r\n",
        "  )\r\n",
        "\r\n",
        "  automl.fit(x_train, y_train[:,i])\r\n",
        "\r\n",
        "  print(automl.show_models())\r\n",
        "\r\n",
        "  pred_y_val.append(automl.predict(x_val))\r\n",
        "\r\n",
        "pred_y_val = np.stack(pred_y_val, axis=-1)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5364, 64, 6)\n",
            "(5364, 63, 2)\n",
            "[WARNING] [2021-01-30 15:04:28,862:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 1. Number of dummy models: 1\n",
            "[WARNING] [2021-01-30 15:04:30,876:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 1. Number of dummy models: 1\n",
            "[WARNING] [2021-01-30 15:04:32,888:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 1. Number of dummy models: 1\n",
            "[WARNING] [2021-01-30 15:04:34,899:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 1. Number of dummy models: 1\n",
            "[WARNING] [2021-01-30 15:04:36,912:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 1. Number of dummy models: 1\n",
            "[WARNING] [2021-01-30 15:04:38,927:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 1. Number of dummy models: 1\n",
            "[WARNING] [2021-01-30 15:04:40,944:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 1. Number of dummy models: 1\n",
            "[WARNING] [2021-01-30 15:04:42,955:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 1. Number of dummy models: 1\n",
            "[WARNING] [2021-01-30 15:04:44,968:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 2. Number of dummy models: 1\n",
            "[WARNING] [2021-01-30 15:04:46,982:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 2. Number of dummy models: 1\n",
            "[WARNING] [2021-01-30 15:04:48,998:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 2. Number of dummy models: 1\n",
            "[WARNING] [2021-01-30 15:04:51,015:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 2. Number of dummy models: 1\n",
            "[WARNING] [2021-01-30 15:04:53,027:EnsembleBuilder(1):7563652e49476cb6ec32da6a9566993e] No models better than random - using Dummy Score!Number of models besides current dummy model: 2. Number of dummy models: 1\n",
            "[(0.920000, SimpleRegressionPipeline({'data_preprocessing:categorical_transformer:categorical_encoding:__choice__': 'one_hot_encoding', 'data_preprocessing:categorical_transformer:category_coalescence:__choice__': 'no_coalescense', 'data_preprocessing:numerical_transformer:imputation:strategy': 'mean', 'data_preprocessing:numerical_transformer:rescaling:__choice__': 'none', 'feature_preprocessor:__choice__': 'no_preprocessing', 'regressor:__choice__': 'gradient_boosting', 'regressor:gradient_boosting:early_stop': 'valid', 'regressor:gradient_boosting:l2_regularization': 0.0004581673561271838, 'regressor:gradient_boosting:learning_rate': 0.32962137015342907, 'regressor:gradient_boosting:loss': 'least_squares', 'regressor:gradient_boosting:max_bins': 255, 'regressor:gradient_boosting:max_depth': 'None', 'regressor:gradient_boosting:max_leaf_nodes': 3, 'regressor:gradient_boosting:min_samples_leaf': 84, 'regressor:gradient_boosting:scoring': 'loss', 'regressor:gradient_boosting:tol': 1e-07, 'regressor:gradient_boosting:n_iter_no_change': 20, 'regressor:gradient_boosting:validation_fraction': 0.1407503228863241},\n",
            "dataset_properties={\n",
            "  'task': 4,\n",
            "  'sparse': False,\n",
            "  'multioutput': False,\n",
            "  'target_type': 'regression',\n",
            "  'signed': False})),\n",
            "(0.080000, SimpleRegressionPipeline({'data_preprocessing:categorical_transformer:categorical_encoding:__choice__': 'no_encoding', 'data_preprocessing:categorical_transformer:category_coalescence:__choice__': 'minority_coalescer', 'data_preprocessing:numerical_transformer:imputation:strategy': 'median', 'data_preprocessing:numerical_transformer:rescaling:__choice__': 'standardize', 'feature_preprocessor:__choice__': 'feature_agglomeration', 'regressor:__choice__': 'gradient_boosting', 'data_preprocessing:categorical_transformer:category_coalescence:minority_coalescer:minimum_fraction': 0.04925167304520271, 'feature_preprocessor:feature_agglomeration:affinity': 'manhattan', 'feature_preprocessor:feature_agglomeration:linkage': 'average', 'feature_preprocessor:feature_agglomeration:n_clusters': 27, 'feature_preprocessor:feature_agglomeration:pooling_func': 'mean', 'regressor:gradient_boosting:early_stop': 'off', 'regressor:gradient_boosting:l2_regularization': 3.0777178597597097e-10, 'regressor:gradient_boosting:learning_rate': 0.11742891344336259, 'regressor:gradient_boosting:loss': 'least_squares', 'regressor:gradient_boosting:max_bins': 255, 'regressor:gradient_boosting:max_depth': 'None', 'regressor:gradient_boosting:max_leaf_nodes': 14, 'regressor:gradient_boosting:min_samples_leaf': 12, 'regressor:gradient_boosting:scoring': 'loss', 'regressor:gradient_boosting:tol': 1e-07},\n",
            "dataset_properties={\n",
            "  'task': 4,\n",
            "  'sparse': False,\n",
            "  'multioutput': False,\n",
            "  'target_type': 'regression',\n",
            "  'signed': False})),\n",
            "]\n",
            "[(0.500000, SimpleRegressionPipeline({'data_preprocessing:categorical_transformer:categorical_encoding:__choice__': 'one_hot_encoding', 'data_preprocessing:categorical_transformer:category_coalescence:__choice__': 'no_coalescense', 'data_preprocessing:numerical_transformer:imputation:strategy': 'mean', 'data_preprocessing:numerical_transformer:rescaling:__choice__': 'none', 'feature_preprocessor:__choice__': 'no_preprocessing', 'regressor:__choice__': 'gradient_boosting', 'regressor:gradient_boosting:early_stop': 'valid', 'regressor:gradient_boosting:l2_regularization': 0.0004581673561271838, 'regressor:gradient_boosting:learning_rate': 0.32962137015342907, 'regressor:gradient_boosting:loss': 'least_squares', 'regressor:gradient_boosting:max_bins': 255, 'regressor:gradient_boosting:max_depth': 'None', 'regressor:gradient_boosting:max_leaf_nodes': 3, 'regressor:gradient_boosting:min_samples_leaf': 84, 'regressor:gradient_boosting:scoring': 'loss', 'regressor:gradient_boosting:tol': 1e-07, 'regressor:gradient_boosting:n_iter_no_change': 20, 'regressor:gradient_boosting:validation_fraction': 0.1407503228863241},\n",
            "dataset_properties={\n",
            "  'task': 4,\n",
            "  'sparse': False,\n",
            "  'multioutput': False,\n",
            "  'target_type': 'regression',\n",
            "  'signed': False})),\n",
            "(0.400000, SimpleRegressionPipeline({'data_preprocessing:categorical_transformer:categorical_encoding:__choice__': 'one_hot_encoding', 'data_preprocessing:categorical_transformer:category_coalescence:__choice__': 'no_coalescense', 'data_preprocessing:numerical_transformer:imputation:strategy': 'most_frequent', 'data_preprocessing:numerical_transformer:rescaling:__choice__': 'robust_scaler', 'feature_preprocessor:__choice__': 'select_rates_regression', 'regressor:__choice__': 'gradient_boosting', 'data_preprocessing:numerical_transformer:rescaling:robust_scaler:q_max': 0.7484939216574421, 'data_preprocessing:numerical_transformer:rescaling:robust_scaler:q_min': 0.20183301399810935, 'feature_preprocessor:select_rates_regression:alpha': 0.18375456889543734, 'feature_preprocessor:select_rates_regression:mode': 'fdr', 'feature_preprocessor:select_rates_regression:score_func': 'f_regression', 'regressor:gradient_boosting:early_stop': 'off', 'regressor:gradient_boosting:l2_regularization': 4.81881052684467e-05, 'regressor:gradient_boosting:learning_rate': 0.10285955822720894, 'regressor:gradient_boosting:loss': 'least_squares', 'regressor:gradient_boosting:max_bins': 255, 'regressor:gradient_boosting:max_depth': 'None', 'regressor:gradient_boosting:max_leaf_nodes': 8, 'regressor:gradient_boosting:min_samples_leaf': 1, 'regressor:gradient_boosting:scoring': 'loss', 'regressor:gradient_boosting:tol': 1e-07},\n",
            "dataset_properties={\n",
            "  'task': 4,\n",
            "  'sparse': False,\n",
            "  'multioutput': False,\n",
            "  'target_type': 'regression',\n",
            "  'signed': False})),\n",
            "(0.060000, SimpleRegressionPipeline({'data_preprocessing:categorical_transformer:categorical_encoding:__choice__': 'no_encoding', 'data_preprocessing:categorical_transformer:category_coalescence:__choice__': 'minority_coalescer', 'data_preprocessing:numerical_transformer:imputation:strategy': 'median', 'data_preprocessing:numerical_transformer:rescaling:__choice__': 'standardize', 'feature_preprocessor:__choice__': 'feature_agglomeration', 'regressor:__choice__': 'gradient_boosting', 'data_preprocessing:categorical_transformer:category_coalescence:minority_coalescer:minimum_fraction': 0.04925167304520271, 'feature_preprocessor:feature_agglomeration:affinity': 'manhattan', 'feature_preprocessor:feature_agglomeration:linkage': 'average', 'feature_preprocessor:feature_agglomeration:n_clusters': 27, 'feature_preprocessor:feature_agglomeration:pooling_func': 'mean', 'regressor:gradient_boosting:early_stop': 'off', 'regressor:gradient_boosting:l2_regularization': 3.0777178597597097e-10, 'regressor:gradient_boosting:learning_rate': 0.11742891344336259, 'regressor:gradient_boosting:loss': 'least_squares', 'regressor:gradient_boosting:max_bins': 255, 'regressor:gradient_boosting:max_depth': 'None', 'regressor:gradient_boosting:max_leaf_nodes': 14, 'regressor:gradient_boosting:min_samples_leaf': 12, 'regressor:gradient_boosting:scoring': 'loss', 'regressor:gradient_boosting:tol': 1e-07},\n",
            "dataset_properties={\n",
            "  'task': 4,\n",
            "  'sparse': False,\n",
            "  'multioutput': False,\n",
            "  'target_type': 'regression',\n",
            "  'signed': False})),\n",
            "(0.040000, SimpleRegressionPipeline({'data_preprocessing:categorical_transformer:categorical_encoding:__choice__': 'no_encoding', 'data_preprocessing:categorical_transformer:category_coalescence:__choice__': 'minority_coalescer', 'data_preprocessing:numerical_transformer:imputation:strategy': 'most_frequent', 'data_preprocessing:numerical_transformer:rescaling:__choice__': 'none', 'feature_preprocessor:__choice__': 'extra_trees_preproc_for_regression', 'regressor:__choice__': 'libsvm_svr', 'data_preprocessing:categorical_transformer:category_coalescence:minority_coalescer:minimum_fraction': 0.010000000000000004, 'feature_preprocessor:extra_trees_preproc_for_regression:bootstrap': 'False', 'feature_preprocessor:extra_trees_preproc_for_regression:criterion': 'mae', 'feature_preprocessor:extra_trees_preproc_for_regression:max_depth': 'None', 'feature_preprocessor:extra_trees_preproc_for_regression:max_features': 0.16371799057722908, 'feature_preprocessor:extra_trees_preproc_for_regression:max_leaf_nodes': 'None', 'feature_preprocessor:extra_trees_preproc_for_regression:min_samples_leaf': 19, 'feature_preprocessor:extra_trees_preproc_for_regression:min_samples_split': 14, 'feature_preprocessor:extra_trees_preproc_for_regression:min_weight_fraction_leaf': 0.0, 'feature_preprocessor:extra_trees_preproc_for_regression:n_estimators': 100, 'regressor:libsvm_svr:C': 152.23753997019216, 'regressor:libsvm_svr:epsilon': 0.4080599256803034, 'regressor:libsvm_svr:kernel': 'rbf', 'regressor:libsvm_svr:max_iter': -1, 'regressor:libsvm_svr:shrinking': 'False', 'regressor:libsvm_svr:tol': 0.00039021643225095956, 'regressor:libsvm_svr:degree': 2, 'regressor:libsvm_svr:gamma': 0.09254299447226481},\n",
            "dataset_properties={\n",
            "  'task': 4,\n",
            "  'sparse': False,\n",
            "  'multioutput': False,\n",
            "  'target_type': 'regression',\n",
            "  'signed': False})),\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqxZ7w1kPwBL",
        "outputId": "bc570d36-1994-4cdd-b7d4-c83b4afa59e9"
      },
      "source": [
        "#Dataset is small enough to process validation data in a single batch\r\n",
        "\r\n",
        "#Should probably freeze batch norm -- val set may have significantly different characterstics --\r\n",
        "#but let's not worry about stuff like that here (removed batch norm to avoid this issue)\r\n",
        "\r\n",
        "print(pred_y_val.shape)\r\n",
        "\r\n",
        "for i, qt in enumerate(qts[-2:]):\r\n",
        "  rms = np.sqrt(np.sum((\r\n",
        "      qt.inverse_transform(pred_y_val[:,i:i+1]) - qt.inverse_transform(y_val[:,i:i+1])\r\n",
        "  )**2))\r\n",
        "\r\n",
        "  if i == 0:\r\n",
        "    print(\"Lake_Level RMS:\", rms, \"liters/sec\")\r\n",
        "  elif i == 1:\r\n",
        "    print(\"Flow_Rate RMS:\", rms, \"metres\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(597, 2)\n",
            "Lake_Level RMS: 4.236360699554449 liters/sec\n",
            "Flow_Rate RMS: 65.72382805968051 metres\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI0guKG4CVvA"
      },
      "source": [
        "# #Feature selection\r\n",
        "# from sklearn.ensemble import ExtraTreesRegressor\r\n",
        "# from sklearn.feature_selection import RFECV\r\n",
        "\r\n",
        "# estimator = ExtraTreesRegressor()\r\n",
        "# estimator.fit(input_features, target_features)\r\n",
        "\r\n",
        "# selector = RFECV(estimator, step=1, cv=5)\r\n",
        "# selector = selector.fit(input_features, target_features[:, 1])\r\n",
        "# print(selector.support_)\r\n",
        "# print(estimator.feature_importances_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeRpwe-rTk_C"
      },
      "source": [
        "# #Apply automatic machine learning to regression\r\n",
        "# import autosklearn.regression\r\n",
        "# automl = autosklearn.regression.AutoSklearnRegressor(\r\n",
        "#     time_left_for_this_task=120,\r\n",
        "#     per_run_time_limit=30,\r\n",
        "#     n_jobs=1\r\n",
        "# )\r\n",
        "# automl.fit(\r\n",
        "#     X_train_transformed, \r\n",
        "#     y_train\r\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrx2jgeoVKy7"
      },
      "source": [
        "# automl.sprint_statistics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbkYXc0dVLCo"
      },
      "source": [
        "# automl.show_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVFRCxadVwT5"
      },
      "source": [
        "# import sklearn.metrics\r\n",
        "# predictions = automl.predict(X_test_transformed)\r\n",
        "# sklearn.metrics.r2_score(y_test, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPvus9J-fppB"
      },
      "source": [
        "# #NOTE: The next two code blocks are for a 1D CNN, which yields surprisingly bad performance\r\n",
        "# #Possibly an issue that needs debugging. Swap to AutoML after\r\n",
        "# #Results were Lake_Level RMS: 34.957657, Flow_Rate RMS: 132.83891\r\n",
        "\r\n",
        "# #Define a model with tunable hyperparameters for Bayesian optimization\r\n",
        "\r\n",
        "# from tensorflow.keras.models import Sequential\r\n",
        "# from tensorflow.keras.layers import Conv1D, BatchNormalization, Activation, Flatten, Dense\r\n",
        "# from tensorflow.keras.optimizers import Adam, SGD\r\n",
        "# from tensorflow.keras.optimizers.schedules import ExponentialDecay\r\n",
        "# from tensorflow.keras.losses import MeanSquaredError\r\n",
        "\r\n",
        "# def build_model(start_hidden=64, batch_size=64, learning_rate=1e-3, epochs=100):\r\n",
        "#     \"\"\"\r\n",
        "#     1D CNN with fully connected layers at the end\r\n",
        "#     \"\"\"\r\n",
        "\r\n",
        "#     #Data augmentation pipeline could be added as part of the model\r\n",
        "#     # ...\r\n",
        "\r\n",
        "#     padding = \"VALID\"\r\n",
        "\r\n",
        "#     #Sequential model maps inputs to target outputs\r\n",
        "#     model = Sequential()\r\n",
        "\r\n",
        "#     #Convolutions to exploit temporal correlation\r\n",
        "#     #Could probably improve by using dilated convolutions\r\n",
        "#     for i in range(int(np.log2(NUM_INPUT_DATA//8))):\r\n",
        "#       print(i)\r\n",
        "#       if not i:\r\n",
        "#         model.add(Conv1D(filters=max(start_hidden*2**2, 512), kernel_size=3, strides=1, \r\n",
        "#                          input_shape=(NUM_INPUT_DATA, x_train.shape[-1]), padding=padding))\r\n",
        "#       else:\r\n",
        "#         model.add(Conv1D(filters=max(start_hidden*2**2, 512), kernel_size=3, strides=1, padding=padding))\r\n",
        "\r\n",
        "#       model.add(Conv1D(filters=max(start_hidden*2**2, 512), kernel_size=3, strides=2, padding=padding))\r\n",
        "#       model.add(BatchNormalization())\r\n",
        "#       model.add(Activation(\"relu\"))\r\n",
        "\r\n",
        "#     model.add(Flatten())\r\n",
        "\r\n",
        "#     #Fully connect to outputs\r\n",
        "#     for _ in range(2):\r\n",
        "#       model.add(Dense(4*start_hidden))\r\n",
        "#       model.add(BatchNormalization())\r\n",
        "#       model.add(Activation(\"relu\"))\r\n",
        "    \r\n",
        "#     model.add(Dense(y_train.shape[-1]))\r\n",
        "\r\n",
        "#     #Optimization\r\n",
        "#     # lr_schedule = ExponentialDecay(\r\n",
        "#     #     initial_learning_rate=learning_rate,\r\n",
        "#     #     decay_steps=NUM_INPUT_DATA, \r\n",
        "#     #     decay_rate=np.exp(np.log(1-end_decay)/epochs), \r\n",
        "#     #     staircase=True\r\n",
        "#     # )\r\n",
        "#     optimizer = Adam(lr=learning_rate)\r\n",
        "#     model.compile(optimizer=optimizer, loss=MeanSquaredError())\r\n",
        "\r\n",
        "#     return model\r\n",
        "\r\n",
        "# # model = build_model()\r\n",
        "# # for l in model.layers:\r\n",
        "# #   print(l.output_shape)\r\n",
        "\r\n",
        "# # model.fit(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz4_hEqoEwgx"
      },
      "source": [
        "# #from skopt import BayesSearchCV #Unfortunately, this is causing a lot of bugs\r\n",
        "# from sklearn.model_selection import GridSearchCV\r\n",
        "# from skopt.space import Real, Categorical, Integer\r\n",
        "\r\n",
        "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\r\n",
        "# from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "\r\n",
        "# from tensorflow.keras.callbacks import LearningRateScheduler\r\n",
        "\r\n",
        "\r\n",
        "# #Trivial search space for (likely) most important hyperparameter\r\n",
        "# search_space = {\r\n",
        "#     \"learning_rate\": [0.01]#(0.01, 0.003, 0.001, 0.0003, 0.0001)\r\n",
        "# }\r\n",
        "\r\n",
        "# #Prepare model for interfacing with sklearn. It's a thin wrapper\r\n",
        "# keras_model = KerasRegressor(build_model)\r\n",
        "\r\n",
        "# #Simple grid search. Could use RandomizedSearchCV of BayesSearchCV for higher-dimensional search space\r\n",
        "# #with 5-fold cross-validation\r\n",
        "# search = GridSearchCV(keras_model, search_space, n_jobs=-1, cv=5, verbose=10)\r\n",
        "\r\n",
        "# scheduler = lambda epoch, lr: 0.97*lr #Vaguely sensible stepwise decay rate\r\n",
        "# lr_callback = LearningRateScheduler(scheduler)\r\n",
        "\r\n",
        "# search = search.fit(\r\n",
        "#     x_train, \r\n",
        "#     y_train,\r\n",
        "#     epochs=200, #Probably far higher than needed.\r\n",
        "#     validation_data=(x_val, y_val),\r\n",
        "#     callbacks=[lr_callback], #EarlyStopping(patience=10),Stop if no improvement for 10 epochs\r\n",
        "#     verbose=2\r\n",
        "# )\r\n",
        "\r\n",
        "# print(search.best_params_)\r\n",
        "# print(search.best_score_)\r\n",
        "# print(search.best_estimator_)\r\n",
        "\r\n",
        "# pred_y_val = search.best_estimator_.predict(x_val)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}